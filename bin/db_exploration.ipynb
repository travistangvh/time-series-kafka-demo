{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/michaelscott/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils import get_global_config, get_waveform_path, get_base_time, get_ending_time, plot_waveform,build_spark_session\n",
    "\n",
    "from pyspark.sql.functions import datediff, to_date, max as max_, lit, col, collect_list, row_number, concat_ws, format_number, concat, monotonically_increasing_id\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.pandas.frame import DataFrame\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import wfdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 03:34:07 WARN Utils: Your hostname, Michaels-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.106 instead (on interface en0)\n",
      "23/04/10 03:34:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/Users/michaelscott/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/michaelscott/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/michaelscott/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-06383d2a-18d0-4fed-a612-e9465e92fc95;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.6.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.8-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.2 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in spark-list\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 9179ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.4.8-1 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.6.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.1.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from spark-list in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.2 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   3   |   3   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      "\n",
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n",
      "\n",
      ":: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-06383d2a-18d0-4fed-a612-e9465e92fc95\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/14ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/10 03:34:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "cfg = get_global_config()\n",
    "spark = build_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 95.0, 96.0, 96.0, 96.0, 96.0, 96.0, 98.0, 96.0, 96.0, 96.0, 96.0, 96.0, 95.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 96.0, 95.0, 96.0, 96.0, 95.0, 95.0, 96.0, 95.0, 96.0, 96.0, 96.0, 95.0, 96.0, 96.0, 95.0, 96.0, 96.0, 95.0, 96.0, 97.0, 95.0, 96.0, 95.0, 96.0, 95.0, 95.0, 95.0, 95.0, 96.0, 96.0, 95.0, 96.0, 95.0, 96.0, 96.0, 96.0, 96.0, 95.0, 95.0, 95.0, 96.0, 95.0, 95.0, 95.0, 96.0, 95.0, 96.0, 96.0, 95.0, 96.0, 96.0, 96.0, 95.0, 95.0, 95.0, 97.0, 95.0, 95.0, 95.0, 95.0, 95.0, 95.0, 95.0, 97.0, 96.0, 96.0, 95.0, 95.0, 95.0, 95.0, 95.0, 95.0, 95.0, 95.0, 96.0, 96.0, 96.0, 96.0, 95.0, 95.0, 95.0, 95.0, 96.0, 95.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(F'file:/{cfg[\"EXPLOREPATH\"]}/X.TESTINPUT')\n",
    "print(list(test_df['HR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: 0 of type <class 'int'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Convert list to array\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m array([  \u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/sql/functions.py:3511\u001b[0m, in \u001b[0;36marray\u001b[0;34m(*cols)\u001b[0m\n\u001b[1;32m   3509\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(cols) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(cols[\u001b[39m0\u001b[39m], (\u001b[39mlist\u001b[39m, \u001b[39mset\u001b[39m)):\n\u001b[1;32m   3510\u001b[0m     cols \u001b[39m=\u001b[39m cols[\u001b[39m0\u001b[39m]  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 3511\u001b[0m \u001b[39mreturn\u001b[39;00m _invoke_function_over_seq_of_columns(\u001b[39m\"\u001b[39;49m\u001b[39marray\u001b[39;49m\u001b[39m\"\u001b[39;49m, cols)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/sql/functions.py:103\u001b[0m, in \u001b[0;36m_invoke_function_over_seq_of_columns\u001b[0;34m(name, cols)\u001b[0m\n\u001b[1;32m    101\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n\u001b[1;32m    102\u001b[0m \u001b[39massert\u001b[39;00m sc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m _invoke_function(name, _to_seq(sc, cols, _to_java_column))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/sql/column.py:86\u001b[0m, in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mConvert a list of Column (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[39mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m converter:\n\u001b[0;32m---> 86\u001b[0m     cols \u001b[39m=\u001b[39m [converter(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m cols]\n\u001b[1;32m     87\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/sql/column.py:86\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mConvert a list of Column (or names) into a JVM Seq of Column.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \n\u001b[1;32m     82\u001b[0m \u001b[39mAn optional `converter` could be used to convert items in `cols`\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39minto JVM Column objects.\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mif\u001b[39;00m converter:\n\u001b[0;32m---> 86\u001b[0m     cols \u001b[39m=\u001b[39m [converter(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m cols]\n\u001b[1;32m     87\u001b[0m \u001b[39massert\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m sc\u001b[39m.\u001b[39m_jvm\u001b[39m.\u001b[39mPythonUtils\u001b[39m.\u001b[39mtoSeq(cols)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/sql/column.py:65\u001b[0m, in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     63\u001b[0m     jcol \u001b[39m=\u001b[39m _create_column_from_name(col)\n\u001b[1;32m     64\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mInvalid argument, not a string or column: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m of type \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFor column literals, use \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlit\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39marray\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mstruct\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcreate_map\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     69\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfunction.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(col, \u001b[39mtype\u001b[39m(col))\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m jcol\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: 0 of type <class 'int'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "# Convert list to array\n",
    "array([  0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mock_df[\u001b[39m'\u001b[39;49m\u001b[39mHR\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mforeach(\u001b[39mlambda\u001b[39;49;00m x: \u001b[39mprint\u001b[39;49m(x))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "#print out all the values of mock_df['HR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, from_json, to_json, col, struct, lower, array\n",
    "\n",
    "# This is the mock data from one patient\n",
    "mock_df = spark.read.csv(F'file:/{cfg[\"EXPLOREPATH\"]}/X.TESTINPUT', header=True)\n",
    "\n",
    "mock_df = mock_df.withColumn(\n",
    "    'structs',\n",
    "    to_json(struct(\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"HR\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"RESP\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"PULSE\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"etco2\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"SpO2\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"CVP\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"AWRR\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"NBP_Mean\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"NBP_Dias\"),\n",
    "        array([lit(95.0) for _ in range(120)]).alias(\"NBP_Sys\")\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             structs|\n",
      "+--------------------+\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "|{\"HR\":[95.0,95.0,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out all values of mock_df['HR']\n",
    "mock_df.select('structs').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'array(HR)'>\n"
     ]
    }
   ],
   "source": [
    "print(array(mock_df['HR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[HR: string, RESP: string, PULSE: string, etco2: string, SpO2: string, CVP: string, AWRR: string, NBP_Mean: string, NBP_Dias: string, NBP_Sys: string, structs: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Cannot resolve column name \"0\" among (HR, RESP, PULSE, etco2, SpO2, CVP, AWRR, NBP_Mean, NBP_Dias, NBP_Sys)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m to_json(struct(\n\u001b[0;32m----> 2\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m'\u001b[39;49m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mHR\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      3\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mRESP\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      4\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m2\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mPULSE\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m3\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39metco2\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m4\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mSpO2\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m5\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mCVP\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m6\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mAWRR\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m7\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mNBP_Mean\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m8\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mNBP_Dias\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     11\u001b[0m \tarray(mock_df[\u001b[39m'\u001b[39m\u001b[39m9\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mNBP_Sys\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m ))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/sql/dataframe.py:1965\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1949\u001b[0m \u001b[39m\"\"\"Returns the column as a :class:`Column`.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \n\u001b[1;32m   1951\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[39m[Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1963\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1964\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, \u001b[39mstr\u001b[39m):\n\u001b[0;32m-> 1965\u001b[0m     jc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mapply(item)\n\u001b[1;32m   1966\u001b[0m     \u001b[39mreturn\u001b[39;00m Column(jc)\n\u001b[1;32m   1967\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, Column):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bd4hproject/lib/python3.9/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"0\" among (HR, RESP, PULSE, etco2, SpO2, CVP, AWRR, NBP_Mean, NBP_Dias, NBP_Sys)"
     ]
    }
   ],
   "source": [
    "\t\t\tto_json(struct(\n",
    "\t\t\t\tarray(mock_df['0']).alias(\"HR\"),\n",
    "\t\t\t\tarray(mock_df['1']).alias(\"RESP\"),\n",
    "\t\t\t\tarray(mock_df['2']).alias(\"PULSE\"),\n",
    "\t\t\t\tarray(mock_df['3']).alias(\"etco2\"),\n",
    "\t\t\t\tarray(mock_df['4']).alias(\"SpO2\"),\n",
    "\t\t\t\tarray(mock_df['5']).alias(\"CVP\"),\n",
    "\t\t\t\tarray(mock_df['6']).alias(\"AWRR\"),\n",
    "\t\t\t\tarray(mock_df['7']).alias(\"NBP_Mean\"),\n",
    "\t\t\t\tarray(mock_df['8']).alias(\"NBP_Dias\"),\n",
    "\t\t\t\tarray(mock_df['9']).alias(\"NBP_Sys\")\n",
    "\t\t\t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (346178703.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 14\u001b[0;36m\u001b[0m\n\u001b[0;31m    ))\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'struct(array(0) AS HR, array(1) AS RESP, array(2) AS PULSE, array(3) AS etco2, array(4) AS SpO2, array(5) AS CVP, array(6) AS AWRR, array(7) AS NBP_Mean, array(8) AS NBP_Dias, array(9) AS NBP_Sys)'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "struct(\n",
    "\t\t\t\tarray(mock_df['0']).alias(\"HR\"),\n",
    "\t\t\t\tarray(mock_df['1']).alias(\"RESP\"),\n",
    "\t\t\t\tarray(mock_df['2']).alias(\"PULSE\"),\n",
    "\t\t\t\tarray(mock_df['3']).alias(\"etco2\"),\n",
    "\t\t\t\tarray(mock_df['4']).alias(\"SpO2\"),\n",
    "\t\t\t\tarray(mock_df['5']).alias(\"CVP\"),\n",
    "\t\t\t\tarray(mock_df['6']).alias(\"AWRR\"),\n",
    "\t\t\t\tarray(mock_df['7']).alias(\"NBP_Mean\"),\n",
    "\t\t\t\tarray(mock_df['8']).alias(\"NBP_Dias\"),\n",
    "\t\t\t\tarray(mock_df['9']).alias(\"NBP_Sys\")\n",
    "\t\t\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_into_pyspark(file, schema=None, cache=False):\n",
    "    if not schema:\n",
    "        df=spark.read.csv(f'file:/{cfg[\"MIMICPATH\"]}/{file}.csv.gz', header=True, inferSchema=True)\n",
    "    else:\n",
    "        df=spark.read.csv(f'file:/{cfg[\"MIMICPATH\"]}/{file}.csv.gz', header=True, schema=schema)\n",
    "    if cache:\n",
    "        df.cache()\n",
    "    df.createOrReplaceTempView(file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "PATIENTS = read_into_pyspark('PATIENTS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ROW_ID: int, SUBJECT_ID: int, GENDER: string, DOB: timestamp, DOD: timestamp, DOD_HOSP: timestamp, DOD_SSN: timestamp, EXPIRE_FLAG: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|SUBJECT_ID|\n",
      "+----------+\n",
      "|      2659|\n",
      "|      3794|\n",
      "|      4900|\n",
      "|      4935|\n",
      "|      7253|\n",
      "|     12799|\n",
      "|     13840|\n",
      "|     15619|\n",
      "|     15727|\n",
      "|     17753|\n",
      "+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find matched patients\n",
    "# The file \"/Users/michaelscott/bd4h/project/data/waveform/physionet.org/files/mimic3wdb-matched/1.0/RECORDS\"\n",
    "# contains a list of all the records in the waveform database.\n",
    "# Each line is \"p00/p000085/\"\n",
    "# Now, extract all the patient IDs that are present in this file\n",
    "# and that will be used to filter all the other tables.\n",
    "\n",
    "from pyspark.sql.functions import substring\n",
    "\n",
    "# Read the RECORDS file as a text file and split each line into patient IDs\n",
    "# convert string to int\n",
    "patient_ids = spark.read.text(f\"file://{cfg['WAVEFPATH']}/RECORDS\") \\\n",
    "                    .select(substring(\"value\", 6, 6).alias(\"SUBJECT_ID\")) \\\n",
    "                    .selectExpr(\"CAST(SUBJECT_ID AS INT) AS SUBJECT_ID\")\\\n",
    "                    .distinct()\n",
    "\n",
    "patient_ids.createOrReplaceTempView('matched_patients')\n",
    "# Show the first 10 patient IDs in the DataFrame\n",
    "patient_ids.show(10)\n",
    "\n",
    "# Use the patient IDs to filter another PySpark table\n",
    "# other_table_filtered = other_table.join(patient_ids, on=\"patient_id\", how=\"inner\")\n",
    "\n",
    "# Get all patient_id from matched_patients as a list\n",
    "patientid = patient_ids.select('SUBJECT_ID').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_age_df = spark.sql(\"\"\"SELECT matched_patients.SUBJECT_ID, date_format(DOB, 'yyyy-MM-dd') dob FROM patients \n",
    "INNER JOIN matched_patients \n",
    "ON matched_patients.subject_id = patients.subject_id\"\"\")\n",
    "\n",
    "# Write df to CSV\n",
    "patients_age_df.coalesce(1).write.csv(f'file://{cfg[\"EXPLOREPATH\"]}/patients_age', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = df.toPandas()\n",
    "\n",
    "# Convert Pandas DataFrame to SQL insert statements\n",
    "insert_sql = pdf.to_sql(table_name, conn, if_exists=\"append\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bd4hproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
